{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ac30d-49ae-4779-81c5-6c6b48e779c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import DataLoader, AortaNormalizer\n",
    "from src.visualiazation import plot_pca\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a5ba8-a529-448f-ba94-3d560dc13b6b",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17d365-6b60-4964-9ded-4ea132fe28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../../data/PulHypStudie_Check_npz_v2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4deb19-0a22-4f0a-9251-e235161d4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [i for i in range(1, 10)]  # (1,11)-> all pigs\n",
    "data_list  # pig 10 -> test pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f28627-1864-4b75-aaba-6cbed334e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data_path)\n",
    "\n",
    "Eit, Y, Pig = data_loader.load_data(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0843c-8175-415e-a47f-ceb569d79c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aorta_normalizer = AortaNormalizer()\n",
    "Y_norm = aorta_normalizer.normalize_forward(Y)\n",
    "Y_true = Y[:, :, 0]\n",
    "\n",
    "assert np.allclose(Y_true, aorta_normalizer.normalize_inverse(Y_norm)[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a07a2-bdd3-4560-821f-8ad5b41cae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(Y_norm.flatten(), bins=100)\n",
    "plt.show()\n",
    "print(np.mean(Y_norm.flatten()))\n",
    "print(np.var(Y_norm.flatten()))\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(Y_true.flatten(), bins=100)\n",
    "plt.show()\n",
    "print(np.mean(Y_true.flatten()))\n",
    "print(np.var(Y_true.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341f767-e01e-46b9-9d11-47f73e52a5b6",
   "metadata": {},
   "source": [
    "**VAE** - HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740c633-c42e-41c4-bcb7-466f5c11b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    Conv1D,\n",
    "    ZeroPadding1D,\n",
    "    Reshape,\n",
    "    Cropping1D,\n",
    ")\n",
    "from tensorflow.keras.layers import Conv1DTranspose\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.total_loss_tracker = Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            loss, reconstruction_loss, kl_loss = self.vae_loss(\n",
    "                data, reconstruction, z_mean, z_log_var\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        loss, reconstruction_loss, kl_loss = self.vae_loss(\n",
    "            data, reconstruction, z_mean, z_log_var\n",
    "        )\n",
    "\n",
    "        self.total_loss_tracker.update_state(loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def vae_loss(self, inputs, outputs, z_mean, z_log_var):\n",
    "        mse_loss_fn = MeanSquaredError()\n",
    "        input_dim = 1024\n",
    "        reconstruction_loss = mse_loss_fn(inputs, outputs) * input_dim\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "\n",
    "# The encoder model\n",
    "def encoder_model(\n",
    "    input_shape=(1024, 1),\n",
    "    channels=(5, 10, 20, 30),\n",
    "    strides=(4, 4, 4, 4),\n",
    "    kernel_size=(5, 5, 5, 5),\n",
    "    latent_dim=8,\n",
    "):\n",
    "    encoder_inputs = Input(shape=input_shape)\n",
    "    x = encoder_inputs\n",
    "\n",
    "    for ch_n, str_n, kernel_s in zip(channels, strides, kernel_size):\n",
    "        x = Conv1D(ch_n, kernel_s, padding=\"same\", strides=1)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "        x = Conv1D(ch_n, kernel_s, padding=\"same\", strides=str_n)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "    z = Sampling()((z_mean, z_log_var))\n",
    "\n",
    "    return encoder_inputs, z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "# The decoder model\n",
    "def decoder_model(\n",
    "    latent_dim=8,\n",
    "    channels=(30, 20, 10, 5),\n",
    "    strides=(4, 4, 4, 4),\n",
    "    kernel_size=(5, 5, 5, 5),\n",
    "):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    L = (1024 // np.prod(strides)) * channels[0]\n",
    "    x = Dense(L, activation=\"elu\")(latent_inputs)\n",
    "    x = Reshape((1024 // np.prod(strides), channels[0]))(x)\n",
    "\n",
    "    for ch_n, str_n, kernel_s in zip(channels, strides, kernel_size):\n",
    "        x = Conv1DTranspose(ch_n, kernel_s, padding=\"same\", strides=str_n)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "        x = Conv1D(ch_n, kernel_s, padding=\"same\", strides=1)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "    x = Conv1DTranspose(1, 1, activation=\"elu\", padding=\"same\")(x)\n",
    "    decoded = x\n",
    "\n",
    "    return latent_inputs, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953ba0f-f871-4daf-853a-fa8854866359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae_model(hp):\n",
    "    latent_dim = hp.Int(\"latent_dim\", min_value=4, max_value=16, step=4)\n",
    "    beta = hp.Float(\"beta\", min_value=0.8, max_value=2.0, step=0.1)\n",
    "    # num_channels_choice = hp.Choice(\"num_channels\", values=[5, 10, 20, 30])\n",
    "    kernel_size = hp.Int(\"kernel_size\", min_value=3, max_value=15, step=1)\n",
    "    strides = hp.Int(\"strides\", min_value=2, max_value=4, step=1)\n",
    "\n",
    "    channels = (5, 10, 20, 30)\n",
    "    # kernel_sizes=(5, 5, 5, 5)\n",
    "    kernel_sizes = [kernel_size] * 4\n",
    "    # strides=(4, 4, 4, 4)\n",
    "    stride_sizes = [strides] * 4\n",
    "\n",
    "    # Build encoder and decoder\n",
    "    encoder_inputs, z_mean, z_log_var, z = encoder_model(\n",
    "        channels=channels,\n",
    "        kernel_size=kernel_sizes,\n",
    "        strides=stride_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "    )\n",
    "    encoder = Model(encoder_inputs, (z_mean, z_log_var, z), name=\"Encoder\")\n",
    "\n",
    "    decoder_inputs, decoder_outputs = decoder_model(\n",
    "        channels=channels[::-1],\n",
    "        kernel_size=kernel_sizes[::-1],\n",
    "        strides=stride_sizes[::-1],\n",
    "        latent_dim=latent_dim,\n",
    "    )\n",
    "    decoder = Model(decoder_inputs, decoder_outputs, name=\"Decoder\")\n",
    "\n",
    "    encoder.summary()\n",
    "    decoder.summary()\n",
    "    vae = VAE(encoder, decoder, beta=beta)\n",
    "\n",
    "    vae.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"lr\", [1e-3, 1e-4, 1e-5])\n",
    "        )\n",
    "    )\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb7920-78a3-4d73-b093-a455b2cf8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_vae_model,\n",
    "    objective=\"loss\",\n",
    "    max_epochs=20,\n",
    "    factor=2,\n",
    "    directory=\"vae_hpt\",\n",
    "    project_name=\"vae_tuning_2\",\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(Y_norm, epochs=10, validation_split=0.1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a47c7-005d-49de-a498-c586f59f7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb74b6-6915-4970-a2ce-973a5d7bb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec6d44-d72e-4759-8767-74c1cf273cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8114f0-6d17-4d8e-9767-96a74ea5d885",
   "metadata": {},
   "source": [
    "**Train hpt model**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b7b0501-37c7-4bcf-b8d5-4a0909a5e695",
   "metadata": {},
   "source": [
    "from src.vae_model import vae_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12aa9cc-7e86-458a-86a0-ac4b33c703fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.encoder.summary()\n",
    "# vae.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3534f1-e8a7-4a38-b020-77e90a4602b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7bb68-046b-4006-83a5-eeffe8ef769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae = vae_model(beta=0.98)\n",
    "vae.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    ")  # , loss=tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "history_vae = vae.fit(\n",
    "    Y_norm,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ecc5b-64e7-4228-b4f2-6bd564671cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_vae.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history_vae.history[\"reconstruction_loss\"], label=\"reconstruction_loss\")\n",
    "plt.legend()\n",
    "# plt.savefig(f\"model/loss_{int(beta*100)}.png\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48cc59-5358-47e1-b23a-a657b36b4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx_num = len(glob(\"src/weights/vae*.weights.h5\"))\n",
    "vae.save_weights(f\"src/weights/vae_model_excl_10_{s_idx_num}.weights.h5\")\n",
    "np.savez(f\"src/weights/vae_model_excl_10_{s_idx_num}.npz\", loss=history_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f25d9f-4fb4-4da9-ae68-4ceea24ffa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saved as src/weights/vae_model_excl_10_{s_idx_num}.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee160cf-e0c6-40f1-bca4-35a56f79fbf9",
   "metadata": {},
   "source": [
    "**VAE test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3496922-a458-4a05-8e81-6420d8ba9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, z_space = vae.encoder.predict(Y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77656a04-47c1-4dad-9cd8-95ab173610d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(z_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4d732-8865-4a8e-826f-92d87239e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.random(size=(10, 8))\n",
    "Aorta_pred = vae.decoder.predict(z)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.plot(aorta_normalizer.normalize_inverse(Aorta_pred)[i, :, 0], label=f\"$z_{i}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
