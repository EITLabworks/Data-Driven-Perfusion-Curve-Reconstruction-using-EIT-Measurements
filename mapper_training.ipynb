{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a8e8f-4d52-424d-916f-b896bb4f5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import DataLoader, AortaNormalizer, lowpass_filter\n",
    "from src.visualiazation import (\n",
    "    plot_pca,\n",
    "    plot_random_predictions,\n",
    "    plot_relative_error_aorta,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50210c-f3e3-4967-831c-add125a553c0",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34396be2-ba76-415d-8afe-372f8d37ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/PulHypStudie_Check_npz_v2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052d6c8-42cb-4814-8834-761103bf240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [i for i in range(1, 10)]\n",
    "data_list  # pig 10 -> test pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf364d6-514f-4cea-813a-ad5acfd840a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data_path)\n",
    "\n",
    "X, Y, Pig = data_loader.load_data(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f49383-ef9b-47e3-b8ed-3866fd063d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aorta_normalizer = AortaNormalizer()\n",
    "Y_norm = aorta_normalizer.normalize_forward(Y)\n",
    "Y_true = Y[:, :, 0]\n",
    "\n",
    "assert np.allclose(Y_true, aorta_normalizer.normalize_inverse(Y_norm)[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3ac25-61ec-4d98-8fb3-d8488f537153",
   "metadata": {},
   "source": [
    "**Load VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389c0f3-3e19-4555-a8a1-d5f3c4aeb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    Conv1D,\n",
    "    ZeroPadding1D,\n",
    "    Reshape,\n",
    "    Cropping1D,\n",
    ")\n",
    "from tensorflow.keras.layers import Conv1DTranspose\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.total_loss_tracker = Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            loss, reconstruction_loss, kl_loss = self.vae_loss(\n",
    "                data, reconstruction, z_mean, z_log_var\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        loss, reconstruction_loss, kl_loss = self.vae_loss(\n",
    "            data, reconstruction, z_mean, z_log_var\n",
    "        )\n",
    "\n",
    "        self.total_loss_tracker.update_state(loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def vae_loss(self, inputs, outputs, z_mean, z_log_var):\n",
    "        mse_loss_fn = MeanSquaredError()\n",
    "        input_dim = 1024\n",
    "        reconstruction_loss = mse_loss_fn(inputs, outputs) * input_dim\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "\n",
    "# The encoder model\n",
    "def encoder_model(\n",
    "    input_shape=(1024, 1),\n",
    "    channels=(5, 10, 20, 30),\n",
    "    strides=(4, 4, 4, 4),\n",
    "    kernel_size=(5, 5, 5, 5),\n",
    "    latent_dim=8,\n",
    "):\n",
    "    encoder_inputs = Input(shape=input_shape)\n",
    "    x = encoder_inputs\n",
    "\n",
    "    for ch_n, str_n, kernel_s in zip(channels, strides, kernel_size):\n",
    "        x = Conv1D(ch_n, kernel_s, padding=\"same\", strides=1)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "        x = Conv1D(ch_n, kernel_s, padding=\"same\", strides=str_n)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "    z = Sampling()((z_mean, z_log_var))\n",
    "\n",
    "    return encoder_inputs, z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "# The decoder model\n",
    "def decoder_model(\n",
    "    latent_dim=8,\n",
    "    channels=(30, 20, 10, 5),\n",
    "    strides=(4, 4, 4, 4),\n",
    "    kernel_size=(5, 5, 5, 5),\n",
    "):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    L = (1024 // np.prod(strides)) * channels[0]\n",
    "    x = Dense(L, activation=\"elu\")(latent_inputs)\n",
    "    x = Reshape((1024 // np.prod(strides), channels[0]))(x)\n",
    "\n",
    "    for ch_n, str_n, kernel_s in zip(channels, strides, kernel_size):\n",
    "        x = Conv1DTranspose(ch_n, kernel_s, padding=\"same\", strides=str_n)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "        x = Conv1D(ch_n, kernel_s, padding=\"same\", strides=1)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"elu\")(x)\n",
    "\n",
    "    x = Conv1DTranspose(1, 1, activation=\"elu\", padding=\"same\")(x)\n",
    "    decoded = x\n",
    "\n",
    "    return latent_inputs, decoded\n",
    "\n",
    "\n",
    "def build_vae_model(hp):\n",
    "    latent_dim = hp.Int(\"latent_dim\", min_value=4, max_value=16, step=4)\n",
    "    beta = hp.Float(\"beta\", min_value=0.1, max_value=2.0, step=0.1)\n",
    "    # num_channels_choice = hp.Choice(\"num_channels\", values=[5, 10, 20, 30])\n",
    "    kernel_size = hp.Int(\"kernel_size\", min_value=3, max_value=9, step=1)\n",
    "    strides = hp.Int(\"strides\", min_value=2, max_value=4, step=1)\n",
    "\n",
    "    channels = (5, 10, 20, 30)\n",
    "    # kernel_sizes=(5, 5, 5, 5)\n",
    "    kernel_sizes = [kernel_size] * 4\n",
    "    # strides=(4, 4, 4, 4)\n",
    "    stride_sizes = [strides] * 4\n",
    "\n",
    "    # Build encoder and decoder\n",
    "    encoder_inputs, z_mean, z_log_var, z = encoder_model(\n",
    "        channels=channels,\n",
    "        kernel_size=kernel_sizes,\n",
    "        strides=stride_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "    )\n",
    "    encoder = Model(encoder_inputs, (z_mean, z_log_var, z), name=\"Encoder\")\n",
    "\n",
    "    decoder_inputs, decoder_outputs = decoder_model(\n",
    "        channels=channels[::-1],\n",
    "        kernel_size=kernel_sizes[::-1],\n",
    "        strides=stride_sizes[::-1],\n",
    "        latent_dim=latent_dim,\n",
    "    )\n",
    "    decoder = Model(decoder_inputs, decoder_outputs, name=\"Decoder\")\n",
    "\n",
    "    encoder.summary()\n",
    "    decoder.summary()\n",
    "    vae = VAE(encoder, decoder, beta=beta)\n",
    "\n",
    "    vae.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"lr\", [1e-3, 1e-4, 1e-5])\n",
    "        )\n",
    "    )\n",
    "    return vae\n",
    "\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_vae_model,\n",
    "    objective=\"loss\",\n",
    "    max_epochs=20,\n",
    "    factor=2,\n",
    "    directory=\"vae_hpt\",\n",
    "    project_name=\"vae_tuning_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199e6cb-6fe3-4bf9-ae61-d20317d6770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f2f0d-bbaf-41a7-bf06-277f4860eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444dd614-2e6b-4b8f-9a1b-27c17f79128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel_model = \"src/weights/vae_model_excl10_0.weights.h5\"\n",
    "# sel_model = \"src/weights/vae_model_excl_none_1.weights.h5\"\n",
    "sel_model = \"src/weights/vae_model_excl_10_2.weights.h5\"\n",
    "\n",
    "# vae = vae_model()\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "vae.load_weights(sel_model)\n",
    "_, _, z = vae.encoder.predict(Y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163f1ad-9196-4faf-bff9-0c3f0da416f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9393735-41ed-429f-97c8-05edf3671c1a",
   "metadata": {},
   "source": [
    "**Train Mapper**\n",
    "\n",
    "- [KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0bd69-338f-49cf-8378-38fdbdc75b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674f32c-97e2-4601-aaae-07bb96649e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    latent_dim = 8\n",
    "    # initialize the sequential model.\n",
    "    model = keras.Sequential()\n",
    "    # input layer\n",
    "    model.add(keras.layers.Input(shape=(64, 1024, 1)))\n",
    "\n",
    "    # tune the number of hidden layers and units in each.\n",
    "    for i in range(1, hp.Int(\"num_layers\", 4, 7)):\n",
    "        print(f\"Init layer {i=}\")\n",
    "        hp_units = hp.Int(\"units_\" + str(i), min_value=2, max_value=16, step=4)\n",
    "        hp_kernel = hp.Int(\"kernel_\" + str(i), min_value=2, max_value=9, step=1)\n",
    "        # stride dim (0,1)\n",
    "        hp_strides_0 = hp.Int(\"units_0_\" + str(i), min_value=1, max_value=4, step=1)\n",
    "        hp_strides_1 = hp.Int(\"units_1_\" + str(i), min_value=2, max_value=4, step=1)\n",
    "        hp_activation = hp.Choice(\n",
    "            \"activation_\" + str(i), values=[\"relu\", \"elu\", \"tanh\"]\n",
    "        )\n",
    "        hp_dropout = hp.Float(\"dropout_\" + str(i), 0, 1.0, step=0.1)\n",
    "\n",
    "        # create layer\n",
    "        model.add(\n",
    "            keras.layers.Conv2D(\n",
    "                hp_units,\n",
    "                hp_kernel,\n",
    "                strides=(hp_strides_0, hp_strides_1),\n",
    "                padding=\"same\",\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation(hp_activation))\n",
    "        model.add(keras.layers.Dropout(hp_dropout))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    # output layer.\n",
    "    model.add(keras.layers.Dense(latent_dim, activation=\"linear\"))\n",
    "\n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=keras.losses.MeanAbsoluteError(),\n",
    "        # loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e8a2a-16ba-4542-98a4-b95bb421cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    factor=2,\n",
    "    directory=\"hpt_mapper_test10\",\n",
    "    project_name=\"hpt_mapper_test10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767783de-8ffa-49eb-a3a9-1131dcdfb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b07489-e870-49d9-9cf9-c6b0bf68247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X, z, epochs=50, batch_size=20, validation_split=0.2, callbacks=[stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8357ca-89fa-490c-b609-3291ff20d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61e357-9756-4fce-b7a5-d3b4e256b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3606a-c31c-4416-904b-1d14608dfbdb",
   "metadata": {},
   "source": [
    "**Load Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a581bf8-9905-47a6-9c41-6b9c3e6c5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f48765-3dc5-4942-824a-b7d34daea7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, z_train, z_valid = train_test_split(\n",
    "    X, z, test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_valid.shape, z_train.shape, z_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a24a89-19eb-4f5e-9e79-5acbe3c1c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    z_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    # callbacks=[es],\n",
    "    validation_data=(X_valid, z_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425b64b-8764-494d-bf5e-7f1f86946167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"src/weights/mapper_model_1.weights.h5\")\n",
    "np.savez(\"src/weights/mapper_model_1_history.npz\", history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f74f5e-e02b-4b31-af64-8adb5fc4772b",
   "metadata": {},
   "source": [
    "**Test model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa797d-4ff6-4657-a100-91b5d1f56fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc508428-ac69-486a-a0e6-a41cc10a03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_t, _ = data_loader.load_data(10, shuffle=False)\n",
    "\n",
    "aorta_normalizer = AortaNormalizer()\n",
    "Y_norm = aorta_normalizer.normalize_forward(Y_t)\n",
    "Y_true = Y_t[:, :, 0]\n",
    "\n",
    "assert np.allclose(Y_true, aorta_normalizer.normalize_inverse(Y_norm)[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d60c0-e05c-4227-bb95-b3154820ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with trained model\n",
    "z_pred = model.predict(X_test)\n",
    "Y_pred = vae.decoder.predict(z_pred)\n",
    "Y_pred = aorta_normalizer.normalize_inverse(Y_pred)[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b91c1-8b17-4f28-9b18-316a058e7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_err_config = {\n",
    "    \"std\": True,\n",
    "    \"var\": False,\n",
    "    \"mean\": True,\n",
    "    \"s_name\": \"images/result_1.png\",\n",
    "}\n",
    "\n",
    "plot_relative_error_aorta(Y_true, Y_pred, **rel_err_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9615f3-faeb-4fc8-ab4d-24a38ecde2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions(Y_true, Y_pred, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02a5ee-1949-4275-8298-4e2dffc1acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LaTeX_export import output_err_for_LaTeX, output_curve_for_LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148d241-6488-4ad3-9b9d-2e3bbc5e2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_err_for_LaTeX(Y_true, Y_pred, f_name=\"err_serious_result_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddd595-60dd-4d1d-a8d9-0c66f1b35651",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_curve_for_LaTeX(Y_true, Y_pred, f_name=\"curve_serious_result_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f8d53-db50-4e6c-bbc1-9c791c641060",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88255df-fad1-4c5d-904a-4dcad24b0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAP = np.min(Y_true, axis=1) - np.min(Y_pred, axis=1)\n",
    "SAP = np.max(Y_true, axis=1) - np.max(Y_pred, axis=1)\n",
    "MAP = np.mean(Y_true, axis=1) - np.mean(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390330f-ec38-4a72-b3bd-dac6dc1fa95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01ccb3-c07d-4755-be03-053f1678c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = {\"DAP\": DAP, \"SAP\": SAP, \"MAP\": MAP}\n",
    "DF = pd.DataFrame(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec28f2a-7995-4640-9569-9ef7e71ae545",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(DF)\n",
    "plt.savefig(\"hist_DapSapMap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a494766-afe3-412e-b5e3-2e0c68358b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(DF)\n",
    "plt.grid()\n",
    "plt.savefig(\"box_DapSapMap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bacd4-eb73-4e01-b026-6fb03fe83012",
   "metadata": {},
   "source": [
    "**Value over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843e567-a806-44f5-b373-188ad1afd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lowpass_filter(np.concatenate(Y_true)[:10_000]), label=\"Pred\")\n",
    "plt.plot(lowpass_filter(np.concatenate(Y_pred)[:10_000]), label=\"True\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f86d29-403b-4bd3-972e-22d32f492c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLaTeX import LinePlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61d311-b89b-4fb2-9ac5-38f801a7b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lplt = LinePlot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb9f03-90b8-45b3-b9dc-ba5706b60a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lplt.add_yvals(lowpass_filter(np.concatenate(Y_true)[: 1024 * 9]), y_name=\"True\")\n",
    "Lplt.add_yvals(lowpass_filter(np.concatenate(Y_pred)[: 1024 * 9]), y_name=\"Pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77ee74-96de-4935-a5a7-08cb347764ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lplt.export(f_name=\"curve_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
